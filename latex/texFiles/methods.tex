The workhorse of this project is the R language.\cite{RLang}
Not only do we implement our own software, we will also be heaviliy reliant on of the functions from the package \code{momentuHMM} \cite{momentuHMM} (version 1.5.5). In addition to \code{momentuHMM} we will make use of a range of auxiliary packages mainly for data wrangling.\cite{tidyverse}\cite{Rcpp} along with other supplementary packages.\cite{gridExtra}\cite{microbenchmark}\cite{ggthemes}\cite{ggmap}\cite{mapview} However, we first introduce the underlying model and its assumptions.
\subsection{Model formulation and estimation}
A hidden markov model consists of two stochastic processes: The state process, $S_t$ and the observation $\mathbf{Y_t}$ process, which are latent and observed respectively. The variation of the HMM we work with is a discrete-time and finite state HMM i.e. $t\in\{1,\dots , T\}$ and $S_t\in\{1,\dots , N\}$ for some $T, N\in\mathbb{N}$. We assume that the state space is determined by an initial probability vector, $\delta = \left(\mathbb{P}(S_1 = 1),\dots \mathbb{P}(S_1 = N)\right)$. Furthermore, it has transition probability matrix
\begin{align}
    \Gamma = \begin{pmatrix}
        \gamma_{11} & \dots &  \gamma_{1N} \\
        \vdots & \ddots & \vdots \\
        \gamma_{N1} & \dots & \gamma_{NN}
    \end{pmatrix} \label{Gamma1}
\end{align}
that is $\gamma_{ij}$ is the probability of jumping from state $i$ to state $j$. We assume that the chain has the markov property. That is the distribution of the state-sequence at time $t$ only depends on the value the sequnece at time $t-1$. For this we have in (\ref{Gamma1}) defined $\gamma_{ij} = \mathbb{P}\left(S_{t+1} = j | S_t = i\right)$. Now, the observed process depends on the value of the state process in that it is drawn from some parametric distributions in which the parameters, $\Theta$, depend on the specific state at time $t$.
A common working assumption is that the observations in $\mathbf{Y}_t$ are conditionally independent given the value of the state sequence, $S_t$ \cite{UncoveringEcologicalState}. That is, say we observe $\mathbf{Y}_t\in\mathbb{R}^d$. We get the factorization
\begin{align}\mathbb{P}(\mathbf{Y}_t | S_t) = \prod_{i = 1}^{d} \mathbb{P}(Y_t^{(i)} | S_t)
\end{align}
Where the superscript refers to the entry in the observed vector. This property is not vital to the following, however, it makes practical implementations of the likelihood easier, whenever we are dealing with observed sequences of more than one variable.
Turning to the estimation itself, we opt for maximum likelihood estimation as our estimation method of choice. Using the notation $S = \{S_1,\dots, S_T\}$ and $\mathbf{y} = \{\mathbf{y}_1,\dots, \mathbf{y}_T\}$ with an HMM as described in the above, we get that the likelihood, $\mathcal{L}_T$, of observing a particular sequence is
\begin{align}
    \mathcal{L}_T = \mathbb{P}(\mathbf{Y} = \mathbf{y}) = \sum_{S_1 = 1}^N\dots \sum_{S_T = 1}^N \mathbb{P}(\mathbf{Y} = \mathbf{y}, S)
\end{align}
by the law of total probability. Invoking the markov property we can write each term as
\begin{align}
    \mathbb{P}(\mathbf{Y} = \mathbf{y}, S) = \mathbb{P}(S_1)\prod_{t = 2}^T \mathbb{P}(S_t | S_{t-1})\prod_{t = 1}^T \mathbb{P}(\mathbf{Y}_t = \mathbf{y}_t | S_t) 
    \label{termsInLikelihood}
\end{align}
However, computing $\mathcal{L}_T$ is intractible even for relatively small values of $T$ and $N$; we are summing $N^T$ terms composed of $2T$ factors giving a time complexity of $\mathcal{O}\left(N^TT\right)$. Now, by proposition 1 in section 2.3.2 \cite{HHMForTimesSeries} this can be drammatically simplified to
\begin{align}
    \mathcal{L}_T = \delta\mathbb{P}(\mathbf{y}_1)\prod_{t = 1}^{T}\Gamma P(\mathbf{y}_t)\mathbf{1}
    \label{likelihoodOpt}
\end{align}
And via a recursive scheme
\begin{align}
    \alpha_1 = \delta\mathbb{P}(\mathbf{y}_1), \quad \alpha_t = \alpha_{t-1}\Gamma\mathbb{P}(\mathbf{y}_1), \: t = 2,\dots, T
\end{align}
known as the forward algorithm, we instead achieve a time complexity of $\mathcal{O}(N^2T)$. In the recursion, the likelihood is found as $\mathcal{L}_T = \alpha_T\mathbf{1}$.\\ To find estimates for the parameters in the model, we use a numerical maximizer to maximize (\ref{likelihoodOpt}) with respect to $\delta, \Gamma$ and $\Theta$. In addition, we draw inference about the underlying hidden states using the algorithm known as the viterbi algorithm. Section 5.3.2 \cite{HHMForTimesSeries}  derives this; the idea is essentially the same as in the likelihood maximization part above. Use the markov property of the model to devise a recursive scheme. In this instance our goal is to maximize $\mathbb{P}(S | \mathbf{Y})$, which by Bayes' theorem is equivalent to maximizing (\ref{termsInLikelihood}). In this case it is possible to devise a recursive scheme for the computation with a time complexity of $\mathcal{O}(NT)$. \cite{HHMForTimesSeries}
\subsection{Assessing the fit}
When we have access to them in simulation settings, we assess the fit via the ground truth parameters and the hidden state sequence. If $\hat{S}$ and $S$ are the estimated and true state sequences respectively. We of course want $P(\hat{S}_t = S_t)$ for some $t$ to be as large as possible. This probability is estimated using the Monte Carlo estimate. In addition, we investigate the parameters of the observed states distributions for biases. When we do the actual modelling, however, we do not have the ground truth or the hidden state sequence. For this part we draw from our experiences from simulation studies, but we also consider the so-called pseudo-residuals. For the $k$th entry in our observed sequence at time $t$ these are defined as \cite{HHMForTimesSeries}
\begin{align}
    r_t^{(k)} = \Phi^{-1}\left(\mathbb{P}\left(Y_t^{(k)} \leq y_t^{(k)}\right)\right) = \Phi^{-1}\left(u_t^{(k)}\right) \label{pseudoresidualsFormula}
\end{align}
The implementation of which is done analagously to momentuHMM. \cite{momentuHMM}
By invoking the quantile transformation theorem twice we see that if the model is correctly specified then $u_t^{(k)}\sim \mathcal{U}(0,1)$ and thus $r_t^{(k)}\sim\mathcal{N}\left(0,1\right)$. We prefer the gaussian pseudo-residuals, due to them allowing us to more easily detect outliers. On top of this, if the HMM properly catches the variation in the data, there should be no autocorrelation in the pseudo-residuals. \cite{HHMForTimesSeries} In addition, we test for the gaussianity be means of the Jarque-Bera test defined as
\begin{align}
    JB = \frac{n}{6}\left(\frac{\hat{\mu}_3^2}{\hat{\sigma}^6}+\frac{1}{4}\left(\frac{\hat{\mu}_4}{\hat{\sigma}^4}-3\right)^2\right)
\end{align}
Where $\hat{\mu_3}, \hat{\mu_4}$ are the Monte carlo estimates of the third- and fourth central moments respectively, whereas $\hat{\sigma}$ is the monte carlo estimate of the standard deviation. Large test statistics are critical for the null hypothesis: The sample comes from a normal distribution. Asymptotically, the test statitic has a $\chi_2^2$-distribution.\cite{tseries}
