Initially, we consider an elemantary hidden markov model on the golden eagle data.\cite{eagleData} The data contains $45.914$ observations of spatiotemporal information for $58$ total tracks divided into $599$ segments. We will use the variables: \texttt{Segment\_ID}, \texttt{Longitude}, \texttt{Latitude}, \texttt{Altitude}, \texttt{horizontal\_steps}. Consecutive observations are collected at one minute intervals. Although, it has has been tidied already, we will still do some minor data wrangling; we calculate the difference in altitude between consecutive observations within segments. Obviously, by doing so we have to discard the first observation for each segment. This leaves us with $45.315$ observations. Apart from this we see no issues with the data: We present summary statistics and density plots of the variables above in appendix \ref{summarStatistics} and there are no unrealistic- og missing values.\\ Using the coordinates we depict the segments with coordinates in the north-eastern United States; the vast-majority of the observations.
\begin{figure}[h]
    \begin{center}
        \includegraphics[scale = .085]{figures/allBirdTracks.jpeg}
        \caption{Longitude and latitude for the $44.730$ observations belonging to segments in the US.}
    \end{center}
\end{figure}
\subsection{Fitting an HMM to the Golden eagle segments}\label{hmmGoldenEagle}
Having dealt with the exploratory analysis of the eagle data, we commence the modelling part. We used a weibull distribution for the horizontal steps and a normal distribution for the vertical steps. An our assumption was that the eagles had three behavioural states. In sum, $X_t \sim \mathcal{W}(k_i, \lambda_i), \; Y_t \sim \mathcal{N}(\mu_i, \sigma_i^2), \; i = 1,2,3$. For the fitting we of course  had to pick some starting values for $(k_i, \lambda_i, \mu_i, \sigma_i^2)$. As with any numerical maximization problem, this was primarily based on heuristics. We tried a few different strategies, however, here we only consider initialization based on the maximum likelihood estimates. For our observed states these are given by\cite{Cohen1965}
\begin{align}
    \hat{\mu} &= \frac{1}{N}\sum_{i = 1}^N x_i, \quad \hat{\sigma}^2 = \frac{1}{N}\sum_{i = 1}^N \left(x_i-\hat{\mu}\right)^2\\
    \frac{1}{N}\sum_{i = 1}^N \log(y_i)&= \frac{\sum_{i = 1}^N y_i^k\log(y_i)}{\sum_{i = 1}^N y_i^k} - \frac{1}{k} \label{kMLE} \\  \hat{\lambda} &= \left(\frac{1}{N} \sum_{i = 1}^N y_i^{\hat{k}}\right)^{\frac{1}{\hat{k}}}
\end{align}
Where we used numerical methods to find the MLE for $k$ in (\ref{kMLE}). We also tried seperating the data first by means of k-means clustering \cite{RLang} and then using the above heuristic on each cluster.
\subsection{Simulating hidden markov models}\label{simHMM}
After completing the initial modelling, we explored methods to enhance the model by relaxing the assumption of conditional independence. To achieve this, we established a framework that facilitated the sampling of hidden Markov models. By fitting the true model to this simulated data under different scenarios, we gained insights into how weakening the assumption leads to improved model fitting. As it was a pivotal part of the simulation and it could not be vectorized, we implemented a fast markov chain sampler in C++ \cite{Rcpp}. This simulates the underlying state sequence up to 100 times faster than a direct R implementation (see figure \ref{markovChainRvRcpp}). Then we drew from a weibull- and normal distribution independently, where the the parameters depend on the state as in \ref{hmmGoldenEagle}. 
\subsection{Sampling correlated variables in the observed state sequence}\label{correlatedVariables}
In the next part we deviated from the regular hidden markov model by allowing correlation in the observed chain. We did this completely analagously to the sampling described in \ref{simHMM}. The only thing we needed was a way to sample correlated variables from arbitrary distributions.
\subsubsection{Sampling correlated variables}
Motivated by us wanting to keep the same marginal distributions as in the initial fitting, we independently developed a method of simulating correlated variables with any parameterized distribution, the procedure is akin to \cite{thomasWard}. Our primary application was of course the weibull and gaussian distributions; the following sampling procedure does however apply to any imaginable parameterized set of distributions.\\ Firstly, sample $X = (X_1, X_2)$ from a bivariate gaussian distribution with mean vector, $\xi = \mathbf{0}$ and covariance matrix 
\begin{align}\Sigma = \begin{pmatrix}
    1 & \rho \\
    \rho & 1
\end{pmatrix}
\end{align}
By this construction $\rho$ is the correlation between $X_1$ and $X_2$. Now, transform the samples in the following way: First transform $X$ bivariate uniform samples $Y = (\Phi(X_1),\Phi(X_2))$. Then let $Z = (G_1(Y_1), G_2(Y))$, where $G_1, G_2$ are quantile functions of parameterized distributions and not necessarily the same. By applying the quantile transformation theorem twice in a back and forth manner, we see that $Z_1$ and $Z_2$ has distributions corresponding to $G_1, G_2$ respectively. Obviously, the correlation is no longer $\rho$ since the maps $\Phi$ and $G_1, G_2$ are non-linear. Obtaining an analytic expression for the correlation of $Z$ is beyond the scope of this project. Therefore, we rely on experimentation to determine the precise correlation. However, our sampling approach consistently results in a correlation we can somewhat control; the value of it is often in the vicinity of $\rho$. Refer table \ref{correlationTable} in appendix \ref{simWeibullGaussian} for an example of this phenomenon. Additionally, we have the desired flexibility to choose our own marginal distributions. Furthermore, this method can be seamlessly combined with \ref{simHMM}. By the parameterized nature of the distribution the observed sequence can just as before directly depend on the state sequence by letting the parameters depend on the value of it. However, finding the general expression for the density of $Z$ is non-trivial task. Although one can find the general expression, we, in our study, settle with deriving the density function of the weibull-normal distribution. The detailed derivation can be found in appendix \ref{weibullGaussianAppendix} Once it is found it is quite easy to optimize with the computation of the density with C++\cite{Rcpp}.
\subsubsection{Imitating the golden eagle data with varying correlation}